{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, image_paths, keypoint_paths, transform=None):\n",
    "        self.image_paths = image_paths  # List of image file paths\n",
    "        self.keypoint_paths = keypoint_paths  # List of skeletal data file paths\n",
    "        self.transform = transform  # Any transformations for images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load skeletal data\n",
    "        keypoints = np.load(self.keypoint_paths[idx])  # Assuming keypoints are stored as numpy arrays\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
    "\n",
    "        return image, keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "class MultimodalSignLanguageModel(nn.Module):\n",
    "    def __init__(self, cnn_output_dim, lstm_hidden_dim, final_dim, num_classes):\n",
    "        super(MultimodalSignLanguageModel, self).__init__()\n",
    "        \n",
    "        # Image branch: CNN for image feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 16 * 16, cnn_output_dim)  # Adjust based on the image size\n",
    "        )\n",
    "        \n",
    "        # Keypoint branch: LSTM for keypoint sequence processing\n",
    "        self.lstm = nn.LSTM(input_size=42, hidden_size=lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Combined output\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cnn_output_dim + lstm_hidden_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, keypoints):\n",
    "        # Process image\n",
    "        image_features = self.cnn(image)\n",
    "        \n",
    "        # Process keypoints\n",
    "        _, (hn, _) = self.lstm(keypoints)\n",
    "        keypoint_features = hn[-1]\n",
    "        \n",
    "        # Concatenate both\n",
    "        combined = torch.cat((image_features, keypoint_features), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Placeholder paths for images and keypoints (replace with your actual file paths)\n",
    "image_paths = [\"path_to_image_1.jpg\", \"path_to_image_2.jpg\"]\n",
    "keypoint_paths = [\"path_to_keypoints_1.npy\", \"path_to_keypoints_2.npy\"]\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = SignLanguageDataset(image_paths, keypoint_paths, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Model parameters\n",
    "cnn_output_dim = 128\n",
    "lstm_hidden_dim = 64\n",
    "final_dim = 256\n",
    "num_classes = 29  # Adjust based on the number of sign classes\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = MultimodalSignLanguageModel(cnn_output_dim, lstm_hidden_dim, final_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "num_epochs = 5  # Adjust based on your needs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, keypoints in data_loader:\n",
    "        # Move data to appropriate device (if using GPU)\n",
    "        images, keypoints = images.float(), keypoints.float()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, keypoints)\n",
    "        labels = torch.randint(0, num_classes, (images.size(0),))  # Random labels (replace with actual labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Placeholder for evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, keypoints in data_loader:\n",
    "            images, keypoints = images.float(), keypoints.float()\n",
    "            outputs = model(images, keypoints)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            labels = torch.randint(0, num_classes, (images.size(0),))  # Replace with actual labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Evaluate on validation DataLoader (replace with actual validation set)\n",
    "evaluate(model, data_loader)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
